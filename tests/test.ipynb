{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "import torch\n",
    "import hidet\n",
    "from hidet.graph import ops\n",
    "\n",
    "# print graph\n",
    "from torch._dynamo import optimize\n",
    "from torch._inductor import graph\n",
    "class CustomGraphLowering(graph.GraphLowering):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gm: torch.fx.GraphModule,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(gm, *args, **kwargs)\n",
    "        print(\"*\"*30 + \"Print Fx Graph\" + \"*\"*30)\n",
    "        gm.graph.print_tabular()\n",
    "graph.GraphLowering = CustomGraphLowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_take(input, dim, index):\n",
    "    a = hidet.from_torch(input)\n",
    "    b = hidet.from_torch(index)\n",
    "    c = ops.transform.take(a, b, axis=dim)\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_index_select():\n",
    "    input = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
    "    index = torch.as_tensor([0], dtype=torch.int32)\n",
    "    dim = 1\n",
    "    b = torch.index_select(input, dim, index)\n",
    "    print(b)\n",
    "\n",
    "    print(test_take(input, dim, index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_concat():\n",
    "    x = torch.rand([3, 4, 5])\n",
    "    y = torch.rand([0])\n",
    "    print(torch.cat([x, y]))\n",
    "\n",
    "    x = hidet.randn([3, 4, 5])\n",
    "    y = hidet.randn([0])\n",
    "    print(ops.concat([x, y], axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_norm():\n",
    "    dim = 0\n",
    "    x = torch.rand([3, 4, 5])\n",
    "    print(x)\n",
    "    y = torch.norm(x, p = 1, dim=dim)\n",
    "    print(\"torch: \", y.shape)\n",
    "    print(y)\n",
    "    print(\"*\"*99)\n",
    "\n",
    "    x_hidet = hidet.from_torch(x)\n",
    "    y_hidet = ops.normalize.lp_norm(x_hidet, p = 1, dim=dim)\n",
    "    print(\"hidet: \", y_hidet.shape)\n",
    "    print(y_hidet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_broadcast_tensor():\n",
    "    x = torch.arange(3).view(1, 1, 3)\n",
    "    y = torch.arange(2).view(2, 1)\n",
    "    a, b = torch.broadcast_tensors(x, y)\n",
    "    print(\"*\"*20, \"torch\", \"*\"*20)\n",
    "    print(\"x: \", x)\n",
    "    print(\"y: \", y)\n",
    "    print(\"a: \", a)\n",
    "    print(\"b: \", b)\n",
    "\n",
    "    x_hidet = hidet.from_torch(x)\n",
    "    y_hidet = hidet.from_torch(y)\n",
    "    a_hidet = ops.broadcast(x_hidet, [2, 3])\n",
    "    b_hidet = ops.broadcast(y_hidet, [2, 3])\n",
    "    print(\"*\"*20, \"hidet\", \"*\"*20)\n",
    "    print(\"a: \", a_hidet)\n",
    "    print(\"b: \", b_hidet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import math\n",
    "def test_torch_digamma():\n",
    "    def digamma_1(x):\n",
    "        if x == 0:\n",
    "            return float('inf')\n",
    "        elif x < 0:\n",
    "            return digamma(1 - x) - math.pi / math.tan(math.pi * x)\n",
    "        else:\n",
    "            result = 0\n",
    "            while x < 8:\n",
    "                result -= 1 / x\n",
    "                x += 1\n",
    "            print(\"result = \", result)\n",
    "            xx = 1 / (x * x)\n",
    "            result += math.log(x) - 0.5 / x - xx * (1 / 12 - xx * (1 / 120 + xx / 252))\n",
    "            return result\n",
    "    \n",
    "    # 0 < x < 8\n",
    "    def compute_2(x):\n",
    "        k = []\n",
    "        for i in range(int(8 - x // 1)):\n",
    "            k.append(1 / (x + i))\n",
    "        \n",
    "        sum_k = sum(k)\n",
    "        # print(\"sum_k: \", sum_k)\n",
    "\n",
    "        x_1 = 8 + x % 1\n",
    "        print(\"x_1: \", x_1)\n",
    "        xx = 1 / (x_1 * x_1)\n",
    "        ret = -sum_k + math.log(x_1) - 0.5 / x_1 - xx * (1 / 12 - xx * (1 / 120 + xx / 252))\n",
    "        return ret\n",
    "\n",
    "    # x > 8\n",
    "    def compute_3(x):\n",
    "        xx = 1 / (x * x)\n",
    "        ret = math.log(x) - 0.5 / x - xx * (1 / 12 - xx * (1 / 120 + xx / 252))\n",
    "        return ret\n",
    "\n",
    "    def compute_1(x):\n",
    "        if x < 8:\n",
    "            return compute_2(x)\n",
    "        else:\n",
    "            return compute_3(x)\n",
    "            \n",
    "\n",
    "    def digamma(x):\n",
    "        # Handle special cases\n",
    "        if x == 0:\n",
    "            return float('-inf')\n",
    "        elif x < 0:\n",
    "            x_1 = 1 - x\n",
    "            return compute_1(x_1) - math.pi / math.tan(math.pi * x)\n",
    "        else:\n",
    "            return compute_1(x)\n",
    "\n",
    "\n",
    "    print(torch.digamma(torch.Tensor([[0.9988, 1.1198, 0.81]])))\n",
    "    print(digamma(0.9988))\n",
    "    print(digamma(1.1198))\n",
    "    print(digamma(0.81))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tan():\n",
    "    # test_torch_digamma()\n",
    "    x = torch.randn([2, 2]).cuda()\n",
    "    \n",
    "    x_hidet = hidet.from_torch(x)\n",
    "    x_hidet = x_hidet.astype(\"f32\")\n",
    "    y = ops.tan(x_hidet)\n",
    "    print(y)\n",
    "    print(torch.tan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hidet_ir_util_broadcastshape():\n",
    "    from hidet.ir.utils.broadcast_utils import broadcast_shapes\n",
    "    s1 = [1, 2, 1]\n",
    "    s2 = [1, 1, 4]\n",
    "    s3 = [3, 1, 1]\n",
    "    bs = broadcast_shapes([s1, s2, s3])\n",
    "    print(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_torch_Tensor_lgamma():\n",
    "    def gamma(x, num_points=10000):\n",
    "        # 定义积分函数\n",
    "        def integrand(t):\n",
    "            return t**(x-1) * math.exp(-t)\n",
    "\n",
    "        # 梯形法则数值积分\n",
    "        integral = 0\n",
    "        dt = 50 / num_points  # 选择合适的积分步长\n",
    "        for i in range(num_points):\n",
    "            integral += integrand(i * dt + dt / 2) * dt\n",
    "\n",
    "        return integral\n",
    "\n",
    "    def lanczos_gamma(x):\n",
    "        # Lanczos近似参数\n",
    "        g = 7\n",
    "        p = [\n",
    "            0.99999999999980993,\n",
    "            676.5203681218851,\n",
    "            -1259.1392167224028,\n",
    "            771.32342877765313,\n",
    "            -176.61502916214059,\n",
    "            12.507343278686905,\n",
    "            -0.13857109526572012,\n",
    "            9.9843695780195716e-6,\n",
    "            1.5056327351493116e-7\n",
    "        ]\n",
    "        # if x < 0.5:\n",
    "        #     return math.pi / (math.sin(math.pi * x) * lanczos_gamma(1 - x))\n",
    "        # else:\n",
    "        #     x -= 1\n",
    "        #     a = p[0]\n",
    "        #     for i in range(1, g + 2):\n",
    "        #         a += p[i] / (x + i)\n",
    "        #     t = x + g + 0.5\n",
    "        #     return math.sqrt(2 * math.pi) * math.pow(t, (x + 0.5)) * math.exp(-t) * a\n",
    "\n",
    "        # x < 0.5\n",
    "        def compute_1(x):\n",
    "            return math.pi / (math.sin(math.pi * x) * compute_2(1 - x))\n",
    "        \n",
    "        # x >= 0.5\n",
    "        def compute_2(x):\n",
    "            x = x - 1\n",
    "            a = p[0]\n",
    "            for i in range(1, g + 2):\n",
    "                a += p[i] / (x + i)\n",
    "            t = x + g + 0.5\n",
    "            return math.sqrt(2 * math.pi) * math.pow(t, (x + 0.5)) * math.exp(-t) * a\n",
    "        \n",
    "        return compute_1(x) if x < 0.5 else compute_2(x)\n",
    "\n",
    "    \n",
    "    # x = torch.Tensor([-1.2])\n",
    "    # print(x.lgamma())\n",
    "\n",
    "    print(math.gamma(1.2))\n",
    "    print(lanczos_gamma(1.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reshape():\n",
    "    def reshape(x, *shape):\n",
    "        return ops.reshape(x, shape)\n",
    "\n",
    "    x = torch.randn([2, 2, 2])\n",
    "    y = x.reshape([2, -1])\n",
    "    print(y)\n",
    "    x_hidet = hidet.from_torch(x)\n",
    "    y_hidet = ops.reshape(x_hidet, [2, -1])\n",
    "    print(reshape(x_hidet, [2, -1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clamp():\n",
    "    def test(x):\n",
    "        return torch.clamp(x, 1, 3)\n",
    "    from hidet.graph.frontend.torch.register_functions import clamp\n",
    "    hidet.torch.dynamo_config.dump_graph_ir(\"hidet_graph\")\n",
    "    x = torch.Tensor([1, 2, 3])\n",
    "    test_opt = torch.compile(test, backend=\"hidet\")\n",
    "    y = test_opt(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_truediv():\n",
    "    def truediv(x, y):\n",
    "        return x / y\n",
    "    opt_div = torch.compile(truediv, backend=\"hidet\")\n",
    "    x_tensor_int = torch.asarray([2, 3, 4], dtype=torch.int)\n",
    "    x_tensor_float = torch.asarray([2.1, 2.2, 2.3], dtype=torch.float)\n",
    "    x_int = int(2)\n",
    "    x_float = float(3.1)\n",
    "    x_l = [x_tensor_int, x_tensor_float, x_int, x_float]\n",
    "\n",
    "    y_tensor_int = torch.asarray([1, 1, 1], dtype=torch.int)\n",
    "    y_tensor_float = torch.asarray([3.1, 3.2, 3.3], dtype=torch.float)\n",
    "    y_int = int(4)\n",
    "    y_float = float(5.2)\n",
    "    y_l = [y_tensor_int, y_tensor_float, y_int, y_float]\n",
    "\n",
    "    for x in x_l:\n",
    "        for y in y_l:\n",
    "            print(\"*\"*20, x.dtype if isinstance(x, torch.Tensor) else type(x), \" div \", y.dtype if isinstance(y, torch.Tensor) else type(y), \"*\"*20)\n",
    "            z = truediv(x, y)\n",
    "            z_hidet = opt_div(x, y)\n",
    "            print(\"eager: \", z)\n",
    "            print(\"hidet: \", z_hidet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_correct():\n",
    "    def func(a):\n",
    "        x = torch.empty(0)\n",
    "        return torch.cat(x, a)\n",
    "\n",
    "    hidet.torch.dynamo_config.correctness_report()\n",
    "    opt_func = torch.compile(func, backend=\"hidet\")\n",
    "    x = torch.asarray([2, 3, 4])\n",
    "    y = torch.asarray([4, 5, 6])\n",
    "    z = opt_func(x)\n",
    "    print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_torch_empty():\n",
    "    x = torch.empty(0)\n",
    "    # x\n",
    "    # y = torch.Tensor([])\n",
    "    # print(x)\n",
    "\n",
    "    x_hidet = hidet.asarray([], dtype=hidet.int32)\n",
    "    y_hidet = hidet.asarray([2,3,4], dtype=hidet.int32)\n",
    "    print(ops.concat([x_hidet, y_hidet], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([[[-0.4590, -0.2242, -0.3001],\n",
      "         [ 0.1112, -0.0772, -0.6744],\n",
      "         [ 1.3309,  1.9713,  0.9461]],\n",
      "\n",
      "        [[-0.2830, -1.7146,  1.0066],\n",
      "         [ 0.0935,  0.6668, -1.2713],\n",
      "         [ 0.9443, -2.2736, -0.2316]],\n",
      "\n",
      "        [[-0.7779, -1.8326,  1.5674],\n",
      "         [-0.6061, -0.1652,  0.1898],\n",
      "         [-0.0588,  0.2220, -0.3239]]])\n",
      "================== x[list[int]] ==================\n",
      "tensor([[[-0.7779, -1.8326,  1.5674],\n",
      "         [-0.6061, -0.1652,  0.1898],\n",
      "         [-0.0588,  0.2220, -0.3239]],\n",
      "\n",
      "        [[-0.4590, -0.2242, -0.3001],\n",
      "         [ 0.1112, -0.0772, -0.6744],\n",
      "         [ 1.3309,  1.9713,  0.9461]],\n",
      "\n",
      "        [[-0.2830, -1.7146,  1.0066],\n",
      "         [ 0.0935,  0.6668, -1.2713],\n",
      "         [ 0.9443, -2.2736, -0.2316]]])\n",
      "tensor([[[-0.7779, -1.8326,  1.5674],\n",
      "         [-0.6061, -0.1652,  0.1898],\n",
      "         [-0.0588,  0.2220, -0.3239]],\n",
      "\n",
      "        [[-0.4590, -0.2242, -0.3001],\n",
      "         [ 0.1112, -0.0772, -0.6744],\n",
      "         [ 1.3309,  1.9713,  0.9461]],\n",
      "\n",
      "        [[-0.2830, -1.7146,  1.0066],\n",
      "         [ 0.0935,  0.6668, -1.2713],\n",
      "         [ 0.9443, -2.2736, -0.2316]]])\n",
      "================== x[list[int|slice]] ==================\n",
      "tensor([[ 0.1112, -0.0772, -0.6744],\n",
      "        [ 0.0935,  0.6668, -1.2713],\n",
      "        [-0.6061, -0.1652,  0.1898]])\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='hidet' raised:\nRuntimeError: name 'idx' is not defined, occurred when interpreting operator.getitem with\n  getitem(tensor(...), (slice(None, None, None), 1))\ngetitem is defined at\n  File \"/root/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/hidet/graph/frontend/torch/register_functions.py\", line 232\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 31\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(opt_func2(x))\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# x_hidet = hidet.from_torch(x)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# idx = [1, 1]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# y = x[idx]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# y_hidet = ops.take(x_hidet, hidet.asarray(idx))\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# print(\"y_hidet: \", y_hidet)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtest_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[65], line 20\u001b[0m, in \u001b[0;36mtest_getitem\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m opt_func2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(func2, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(func2(x))\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mopt_func2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:655\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py:727\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_entry, hooks, frame_state)\u001b[0m\n\u001b[1;32m    725\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minner_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py:383\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_entry, hooks, frame_state)\u001b[0m\n\u001b[1;32m    370\u001b[0m signpost_event(\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     },\n\u001b[1;32m    380\u001b[0m )\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpatch(_patch_config_if_changed()):\n\u001b[0;32m--> 383\u001b[0m     compiled_product \u001b[38;5;241m=\u001b[39m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_product\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py:646\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 646\u001b[0m         guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    649\u001b[0m         Unsupported,\n\u001b[1;32m    650\u001b[0m         TorchRuntimeError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m         BisectValidationException,\n\u001b[1;32m    658\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/utils.py:244\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    243\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 244\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    246\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py:562\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    560\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py:1033\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1030\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1031\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1033\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py:151\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py:527\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 527\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    529\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py:2128\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2128\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py:818\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m--> 818\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m     ):\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py:781\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    778\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[1;32m    779\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[1;32m    780\u001b[0m     )\n\u001b[0;32m--> 781\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py:2243\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2238\u001b[0m _step_logger()(\n\u001b[1;32m   2239\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   2240\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (RETURN_VALUE)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2241\u001b[0m )\n\u001b[1;32m   2242\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2243\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   2247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_return_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/output_graph.py:919\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason, compile_return_value)\u001b[0m\n\u001b[1;32m    916\u001b[0m     append_prefix_insts()\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 919\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[1;32m    921\u001b[0m     )\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     graph_output_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_var(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/output_graph.py:1087\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[0;32m-> 1087\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m   1090\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_graphs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/utils.py:244\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    243\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 244\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    246\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/output_graph.py:1159\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m   1160\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m   1161\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m signpost_event(\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     },\n\u001b[1;32m   1172\u001b[0m )\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/output_graph.py:1140\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[1;32m   1139\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1140\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1141\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/_dynamo/repro/after_dynamo.py:117\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/torch/__init__.py:1707\u001b[0m, in \u001b[0;36m_TorchCompileWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/hidet/graph/frontend/torch/dynamo_backends.py:147\u001b[0m, in \u001b[0;36mhidet_backend\u001b[0;34m(graph_module, example_inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidet:   \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, arg)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# symbolic run to get flow graph\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43minterpreter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m output_format, output_tensors \u001b[38;5;241m=\u001b[39m serialize_output(output)\n\u001b[1;32m    149\u001b[0m input_tensors \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, hidet\u001b[38;5;241m.\u001b[39mTensor)]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/hidet/graph/frontend/torch/interpreter.py:209\u001b[0m, in \u001b[0;36mInterpreter.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/hidet/graph/frontend/torch/interpreter.py:367\u001b[0m, in \u001b[0;36mInterpreter.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    365\u001b[0m             hidet_env[\u001b[38;5;28mstr\u001b[39m(node\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m])] \u001b[38;5;241m=\u001b[39m hidet_env[node\u001b[38;5;241m.\u001b[39mname]\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 367\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexec_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidet_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidet_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_method\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    369\u001b[0m     args \u001b[38;5;241m=\u001b[39m load_arg(node\u001b[38;5;241m.\u001b[39margs, hidet_env)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/hidet/graph/frontend/torch/interpreter.py:320\u001b[0m, in \u001b[0;36mInterpreter._raise_exception\u001b[0;34m(exception, target, caused_callable, args, kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     caused_callable \u001b[38;5;241m=\u001b[39m dispatched\n\u001b[1;32m    319\u001b[0m callable_name, filename, lineno \u001b[38;5;241m=\u001b[39m Interpreter\u001b[38;5;241m.\u001b[39m_callable_info(caused_callable)\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, occurred when interpreting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(argument_strings)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is defined at\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  File \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlineno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexception\u001b[39;00m\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: backend='hidet' raised:\nRuntimeError: name 'idx' is not defined, occurred when interpreting operator.getitem with\n  getitem(tensor(...), (slice(None, None, None), 1))\ngetitem is defined at\n  File \"/root/miniconda3/envs/pytorch2_2_1/lib/python3.8/site-packages/hidet/graph/frontend/torch/register_functions.py\", line 232\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "def test_getitem():\n",
    "    x = torch.randn([3, 3, 3])\n",
    "    print(\"x: \", x)\n",
    "    \n",
    "    print(\"================== x[list[int]] ==================\")\n",
    "    def func1(x):\n",
    "        idx = [2, 0, 1]\n",
    "        return x[idx]    \n",
    "    opt_func1 = torch.compile(func1, backend=\"hidet\")\n",
    "\n",
    "    print(func1(x))\n",
    "    print(opt_func1(x))\n",
    "    \n",
    "    print(\"================== x[list[int|slice]] ==================\")\n",
    "    def func2(x):\n",
    "        return x[:, 1]\n",
    "    \n",
    "    opt_func2 = torch.compile(func2, backend=\"hidet\")\n",
    "    print(func2(x))\n",
    "    print(opt_func2(x))\n",
    "\n",
    "\n",
    "    # x_hidet = hidet.from_torch(x)\n",
    "    # idx = [1, 1]\n",
    "    # y = x[idx]\n",
    "    # print(\"y: \", y)\n",
    "\n",
    "    # y_hidet = ops.take(x_hidet, hidet.asarray(idx))\n",
    "    # print(\"y_hidet: \", y_hidet)\n",
    "\n",
    "test_getitem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cpu task \u001b[92mreshape(x=float32(3, 1, 3), y=float32(3, 3), shape=[3, 3])\u001b[0m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([[[ 0.6180,  0.2747,  0.0779],\n",
      "         [-0.9967, -1.8447,  1.3099],\n",
      "         [ 0.3335,  0.1612,  0.5073]],\n",
      "\n",
      "        [[-0.5005, -1.6400, -1.3435],\n",
      "         [-0.6357,  1.2300, -0.6855],\n",
      "         [ 0.7223,  2.3741, -0.6154]],\n",
      "\n",
      "        [[-0.3486,  0.1112, -0.2765],\n",
      "         [-2.8281, -0.1209, -0.0776],\n",
      "         [-0.1415, -0.4260, -0.5322]]])\n",
      "torch:  tensor([[ 0.6180,  0.2747,  0.0779],\n",
      "        [-0.5005, -1.6400, -1.3435],\n",
      "        [-0.3486,  0.1112, -0.2765]])\n",
      "hidet:  Tensor(shape=(3, 3), dtype='float32', device='cpu')\n",
      "[[ 0.6179514   0.27466476  0.07788834]\n",
      " [-0.500519   -1.6399834  -1.3435045 ]\n",
      " [-0.34859186  0.11118111 -0.27647382]]\n"
     ]
    }
   ],
   "source": [
    "def test_slice():\n",
    "    x = torch.randn([3, 3, 3])\n",
    "    x_hidet = hidet.from_torch(x)\n",
    "    print(\"x: \", x)\n",
    "    print(\"torch: \", x[[slice(None, None), 0]])\n",
    "    y_hidet = ops.reshape(ops.strided_slice(x_hidet, [None, 0], [None, 1]), [3, 3])\n",
    "    print(\"hidet: \", y_hidet)\n",
    "\n",
    "test_slice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([[[ 0.5130, -0.8551, -1.0871],\n",
      "         [-1.0447,  1.8052,  0.1656],\n",
      "         [-1.2294,  0.5877,  0.5736]],\n",
      "\n",
      "        [[-0.6809,  0.0297, -0.8670],\n",
      "         [-0.1760, -0.2163, -0.3050],\n",
      "         [-0.5036,  1.6644, -0.2396]],\n",
      "\n",
      "        [[-0.1223, -0.9100, -0.1767],\n",
      "         [-1.3702,  0.1823, -0.8587],\n",
      "         [-0.0469,  0.2566,  0.9508]]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x[:, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# print(x[:, 0])\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtest_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m, in \u001b[0;36mtest_getitem\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m x_hidet \u001b[38;5;241m=\u001b[39m hidet\u001b[38;5;241m.\u001b[39mfrom_torch(x)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx: \u001b[39m\u001b[38;5;124m\"\u001b[39m, x)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "def test_getitem():\n",
    "    x = torch.randn([3,3,3])\n",
    "    x_hidet = hidet.from_torch(x)\n",
    "    print(\"x: \", x)\n",
    "    print(x[:, 1, 2])\n",
    "    # print(x[:, 0])\n",
    "\n",
    "\n",
    "\n",
    "test_getitem()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2_2_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
